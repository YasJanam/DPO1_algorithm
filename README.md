# DPO1_algorithm
الگوریتم dpo الگوریتمی هست که عموما در مباحث کاربرد های rl در nlp دیده میشه ولی در حقیقت یک الگوریتم rl نیستش.یک الگوریتم به روز رسانی پارامتر های معمولی هست فقط نوع داده اش از نوع داده ترجیحاته!!
یعنی چی ؟ یعنی اینکه مثل الگوریتم های نظارت شده یک تابع زیان داره که همواره هدفش اینه مقدارش رو کمتر کنه.ولی داده اش مثل الگوریتم های با نظارت نیست (مثلا اینکه برچسب دار باشه), بلکه به این شکله : یک prompt با دو پاسخ, یکی پاسخ ترجیح داده شده هستش و یکی دیکه پاسخ رد شده(یا نامناسب) هست.
# دیتاست
دیتاست هایی که توی الگوریتم dpo استفاده میشوند به شکل سه ستونی هستند : prompt, chosen, rejected
chosen  : پاسخی که انسان ترجیح داده
rejected : پاسخ نامناسب  یا رد شده 

 این سه ستون حتما باید توی دیتاست با همین نامها باشند. 
 
 # مدل
 در الگوریتم dpo  دو مدل داریم. یکی __مدل مرجع__ و یکی دیگه __مدل جدید__ (یا همون مدلی که میخایم آموزشش بدیم)
 مدل مرجع معمولا مدلیه که قویه و قبلا خوب آموزش دیده
 
 # دلایل rl نبودن dpo
 الگوریتم dpo از نظر فنی یک الگوریتم یادگیری تقویتی حساب نمیاد.زیرا : 
 هیچ محیطی,عامل(agent) یا اپیزود تعاملی وجود ندارد
 هیچ تابع ارزش یا مدل پاداش جداگانه ای ساخته نمیشه
 فرآیندش کاملا شبیه یک به روز رسانی پارامتر ها در __یادگیری نظارت شده__ است.فقط داده اش از نوع ترجیحاته

 # dpo loss function :
 
  loss = -logsigmoid( B( logp(A|x) - logq(A|x) ) + B(logp(C|x) - logq(C|x) ) )
  
 x = پرامت ورودی
 A = پاسخ ترجیحی
 C = پاسخ غیر ترجیحی
 B = همون بتا در فرمول تابع زیان الگوریتم
 p = برای مدل در حال آموزش
 q = برای مدل مرجع
p(A|x) = احتمال انتخاب پاسخ ترجیحی توسط مدل در حال آموزش
q(A|x) = احتمال انتخاب پاسخ ترجیحی توسط مدر مرجع
p(C|x) = احتمال انتخاب پاسخ رد شده توسط مدل در حال آموزش
q(C|x) = احتمال انتخاب مپاسخ رد شده توسط مدل مرجع
logsigmoid = لگاریتم تابع سیگموئید , برای نگه داشتن زیان بین 0 و 1
