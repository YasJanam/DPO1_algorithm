# -*- coding: utf-8 -*-
"""DPO2_withPrompt _report.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/157oERw8Ib1VyjfgQVbHdlVWcONWmWgQT
"""

"""
توجه » مدل مرجع و مدل اصلی یکی گرفته شده اند

این کد مثالی از پیاده سازی الگوریتم است و برای اینکه تمرکز روی الگوریتم باشد کلا از یک مدل استفاده کرده ایم ولی این کار باعث میشود زیان بشدت کم شود 

"""

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm
from torch.optim import Adam

raw_dataset = load_dataset("Dahoas/synthetic-instruct-gptj-pairwise",split="train[:10%]")

dataset = DPODataset(raw_dataset)

"""
کلاسی برای اینکه دیتاست را به فرمی که میخواهیم دربیاوریم
"""
class DPODataset(Dataset):
  def __init__(self,hf_dataset):
    self.dataset = hf_dataset

  def __len__(self):
    return len(self.dataset)

  def __getitem__(self,idx):
    item = self.dataset[idx]
    return{
        "prompt": item["prompt"],
        "chosen": item["chosen"],
        "rejected": item["rejected"]
    }

"""
آپلود مدلی که قرار است الگوریتم dpo روی آن آموزش ببیند

"""
model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

"""
این کد یک مثال ساده از پیاده سازی الگوریتم است .برای اینکه تمرکز روی الگوریتم باشد مدل مرجع و مدل در حال آموزش را یکی میگیریم

ولی توجه کنید اینکار باعث میشود زیان بشدت کم باشد!! زیرا هر دو مدل یکی هستن و به همین علت مدل اصلی خوب آموزش نمیبیند

"""
model = AutoModelForCausalLM.from_pretrained(model_name)
ref_model = AutoModelForCausalLM.from_pretrained(model_name)

class DPOTrainer:
  def __init__(self,model,ref_model,tokenizer,beta=0.1,lr=1e-5,device="cuda"):
    self.model=model.to(device)
    self.ref_model = ref_model.to(device)
    self.tokenizer = tokenizer
    self.beta = beta
    self.device = device
    self.optimizer = Adam(self.model.parameters(),lr=lr)

    for p in self.ref_model.parameters():
      p.requires_grad = False
    self.ref_model.eval()

"""
prepare_inputs :

این متد هر پرامپت را با پاسخش کنار هم قرار میدهد و سپس همه انها را توکنایز میکند

"""

  def prepare_inputs(self,prompts,responses):
    texts = [p + r for p,r in zip(prompts,responses)]
    inputs = self.tokenizer(texts,return_tensors="pt",padding=True,truncation=True).to(self.device)
    return inputs


  def compute_log_probs(self,model,inputs):
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    outputs = model(input_ids=input_ids,attention_mask=attention_mask)
    logits = outputs.logits[:,:-1,:]
    log_probs = torch.nn.functional.log_softmax(logits,dim=-1)

    target_ids = input_ids[:,1:]
    mask = attention_mask[:,1:].bool()
    selected_log_probs = log_probs.gather(2,target_ids.unsqueeze(-1)).squeeze(-1)
    selected_log_probs = selected_log_probs * mask

    return selected_log_probs.sum(dim=1)

  def dpo_loss(self,prompts,chosen,rejected):
    # پیاده سازی تابع زیان الگوریتم

    """
    یک بار با پاسخ های ترجیحی و یک بار با پاسخ های رد شده توکنایز را انجام میدهیم

    """
    inputs_chosen = self.prepare_inputs(prompts,chosen)
    inputs_rejected = self.prepare_inputs(prompts,rejected)

    logp_chosen = self.compute_log_probs(self.model,inputs_chosen)
    logp_rejected = self.compute_log_probs(self.model,inputs_rejected)

    with torch.no_grad():
      logp_ref_chosen = self.compute_log_probs(self.ref_model,inputs_chosen)
      logp_ref_rejected = self.compute_log_probs(self.ref_model,inputs_rejected)

    diff_model = logp_chosen - logp_rejected
    diff_ref = logp_ref_chosen - logp_ref_rejected

    loss = -torch.nn.functional.logsigmoid(self.beta * (diff_model - diff_ref)).mean()
    return loss

  def train(self,dataset,epochs=3,batch_size=4):
    dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=True)

    for epoch in range(epochs):
      total_loss = 0
      for batch in tqdm(dataloader,desc=f"Epoch {epoch+1}"):
        prompt = batch["prompt"]
        chosen = batch["chosen"]
        rejected = batch["rejected"]

        loss = self.dpo_loss(prompt,chosen,rejected)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        total_loss += loss.item()

      avg_loss = total_loss / len(dataloader)
      print(f"Epoch {epoch+1} - Loss: {avg_loss:.4f}")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
trainer = DPOTrainer(model,ref_model,tokenizer,device=device)
trainer.train(dataset,epochs=3,batch_size=2)
